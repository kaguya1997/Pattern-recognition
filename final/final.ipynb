{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1681cbe-8ed3-4e14-b20b-f124f1c1f361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\sample_submission.csv.zip\n",
      "input\\test.csv.zip\n",
      "input\\test_labels.csv.zip\n",
      "input\\train.csv.zip\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df0c91f-7112-47a7-92ac-c365c181b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk\n",
    "import texthero as hero\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bab45d2-cf87-438d-a8f0-7125902408c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in g:\\anaconda20211117\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in g:\\anaconda20211117\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in g:\\anaconda20211117\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in g:\\anaconda20211117\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in g:\\anaconda20211117\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: requests in g:\\anaconda20211117\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in g:\\anaconda20211117\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in g:\\anaconda20211117\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in g:\\anaconda20211117\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in g:\\anaconda20211117\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\anaconda20211117\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in g:\\anaconda20211117\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in g:\\anaconda20211117\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: six in g:\\anaconda20211117\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in g:\\anaconda20211117\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in g:\\anaconda20211117\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.4.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f8c637-632e-4753-9284-c19d55c5a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"input/train.csv.zip\")\n",
    "test_df = pd.read_csv(\"input/test.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cd0b6a-71a1-4d34-854e-9f2b94c58500",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = train_df.columns[2:]\n",
    "feature_col = train_df.columns[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9bb0c8-09c6-4475-80d2-7030c8c0c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preprocess(df, col):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    df[col] = (df[col].pipe(hero.lowercase).\n",
    "                       pipe(hero.remove_urls).\n",
    "                       pipe(hero.remove_digits).\n",
    "                       pipe(hero.remove_punctuation).\n",
    "                       pipe(hero.remove_html_tags))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7c2ea3-c47c-4365-b1dd-c44a251dfabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    pre_train_df = df_preprocess(train_df, feature_col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a953dbdc-f785-4b04-ad4d-a11627755b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_test_df = df_preprocess(test_df, feature_col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6587764-9a45-4c8c-8b19-786a24416883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(model_selected):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_selected)\n",
    "  return tokenizer\n",
    "\n",
    "def data_tokenization(dataset, col, max_len, tokenizer):\n",
    "    tokens = dataset[col].apply(\n",
    "        lambda x: tokenizer(x,return_tensors = 'tf', \n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            max_length = max_len, \n",
    "                            add_special_tokens = True))\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for i in tokens:\n",
    "        input_ids.append(i['input_ids'])\n",
    "        attention_mask.append(i['attention_mask'])\n",
    "    input_ids, attention_mask = np.squeeze(input_ids), np.squeeze(attention_mask)\n",
    "    return [input_ids,attention_mask]\n",
    "\n",
    "def bert_model(model_selected, max_len, learning_rate):\n",
    "  bert = TFAutoModel.from_pretrained(model_selected)\n",
    "  for layer in bert.layers:\n",
    "      layer.trainable = True\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07, amsgrad = False, name = 'Adam')\n",
    "  input_ids = Input(shape = (max_len,),dtype = tf.int32,name = 'input_ids')\n",
    "  attention_mask = Input(shape = (max_len,), dtype = tf.int32,name = 'attention_mask')\n",
    "  bert = bert(input_ids, attention_mask)\n",
    "  x = bert[0][:,0,:]\n",
    "  x = tf.keras.layers.Dropout(0.1)(x)\n",
    "  x = tf.keras.layers.Dense(128)(x)\n",
    "  x = tf.keras.layers.Dense(64)(x)\n",
    "  x = tf.keras.layers.Dense(32)(x)\n",
    "  output = tf.keras.layers.Dense(6, activation = 'relu')(x)\n",
    "  model = Model(inputs = [input_ids,attention_mask], outputs = [output])\n",
    "  model.compile(optimizer = optimizer,\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE, name = 'binary_crossentropy'),\n",
    "                metrics = ['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a121e240-39ee-43ac-adb4-55eb75cb3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selected = 'bert-base-uncased'\n",
    "max_len = 256\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b601ea1a-a001-4e64-a273-2c59652ffa53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe9c9aa47ff4a2d9ab08ce5e7881811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1f316de50740f890ad3ddb1d140839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3d753e9944f18b8ee044b3887c06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dc8f038d3342ddac9d523e94beb417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(model_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46099ef3-1900-44cd-b541-8c509b73b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_tokenization(pre_train_df, feature_col[0], max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1db56d7f-6e9a-45f6-a6b8-dee8fb6f7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pre_train_df[target_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e22a8e-bd74-4c78-9efc-8561ac4caf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd899f4bab644b33b3ea6cf280f506d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 256,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 768)          0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6)            198         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,591,206\n",
      "Trainable params: 109,591,206\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert = bert_model(model_selected, max_len, learning_rate)\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bba39db9-3a6b-45ea-ad3b-3e19d5617de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "39893/39893 [==============================] - 106366s 3s/step - loss: 0.6933 - accuracy: 0.9941\n",
      "Epoch 2/2\n",
      "39893/39893 [==============================] - 106452s 3s/step - loss: 0.6933 - accuracy: 0.9942\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    bert.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7a8ee1a-aaaa-4045-9c98-52c9e5bb965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = pre_test_df['id']\n",
    "x_test = data_tokenization(pre_test_df, feature_col[0], max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ac5b95c-1807-4a3e-86e8-9b85e29f56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bert.predict(x_test)\n",
    "submiss_df = pd.DataFrame(preds, columns = target_col)\n",
    "submiss_df['id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f165c0-19d5-4076-85fb-19a07b1caa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss_df.to_csv('submissioin.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a52b2-9d8b-4185-acd2-a80382ae8501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
